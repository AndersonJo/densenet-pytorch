{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "from densenet import DenseNet\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "PRINT_TRAINING_INTERVAL = 1000\n",
    "CUDA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "train_x: (50000, 32, 32, 3)\n",
      "train_y: (50000,)\n",
      "test_x: (10000, 32, 32, 3)\n",
      "test_y: (10000,)\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "\n",
    "to_pil = transforms.ToPILImage()\n",
    "\n",
    "\n",
    "trainset = CIFAR10('./CIFAR10', train=True,  transform=transform, download=True)\n",
    "testset  = CIFAR10('./CIFAR10', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "\n",
    "\n",
    "train_x = trainset.train_data\n",
    "train_y = np.array(trainset.train_labels)\n",
    "test_x = testset.test_data\n",
    "test_y = np.array(testset.test_labels)\n",
    "\n",
    "print('train_x:', train_x.shape)\n",
    "print('train_y:', train_y.shape)\n",
    "print('test_x:', test_x.shape)\n",
    "print('test_y:', test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def show_samples(x, y):\n",
    "    # Convert Tensor to Numpy Array\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        x = x.cpu().numpy().transpose(0, 2, 3, 1)\n",
    "        x = (x - x.min()) / (x.max() - x.min())\n",
    "        x = 1 - x\n",
    "        x *= 255\n",
    "        \n",
    "        x = x.astype('int').astype('float32')\n",
    "        y = y.cpu().numpy()\n",
    "    \n",
    "    x, y = np.array(x), np.array(y)\n",
    "    labels = np.unique(y)\n",
    "    \n",
    "    fig, subplots = pylab.subplots(len(labels), 14, figsize=(14, 10)) # subplots(y축, x축 갯수)\n",
    "    subplots = subplots.T.reshape(-1)\n",
    "\n",
    "    for i, p in enumerate(subplots):\n",
    "        target = x[y == labels[i%len(labels)]]\n",
    "\n",
    "        idx = np.random.randint(target.shape[0])\n",
    "        d = target[idx]\n",
    "        p.get_xaxis().set_visible(False)\n",
    "        p.get_yaxis().set_visible(False)\n",
    "        p.imshow(d)\n",
    "        \n",
    "show_samples(test_loader.dataset.test_data, test_loader.dataset.test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DenseNet(24, 0.5, n_class=10, fc_size=16320, blocks=[24, 24, 24])\n",
    "model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = {'loss': list(), 'duration': list()}\n",
    "step = 0\n",
    "\n",
    "for x_sample, y_sample in train_loader:\n",
    "    start_t = datetime.now()\n",
    "    x_sample, y_sample = Variable(x_sample).cuda(), Variable(y_sample).cuda()\n",
    "    \n",
    "    # Init Gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Feedforward    \n",
    "    y_pred = model(x_sample)\n",
    "    loss = criterion(y_pred, y_sample)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Add Step\n",
    "    step += 1\n",
    "    \n",
    "    # Logging\n",
    "    loss = loss.data.cpu().numpy()[0]\n",
    "    duration = (datetime.now() - start_t).total_seconds()\n",
    "    logs['loss'].append(loss)\n",
    "    logs['duration'].append(duration)\n",
    "    \n",
    "    \n",
    "    if step % PRINT_TRAINING_INTERVAL == 0:\n",
    "        loss = np.mean(logs['loss'][-PRINT_TRAINING_INTERVAL:])\n",
    "        duration = np.mean(logs['duration'][-PRINT_TRAINING_INTERVAL:])\n",
    "        print(f'[{step}] {duration:<6.2} - loss:{loss:<8.4}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_t = datetime.now()\n",
    "n_total_correct = 0\n",
    "total_duration = 0\n",
    "N = len(test_loader.dataset)\n",
    "\n",
    "for i, (x_sample, y_true) in enumerate(test_loader):\n",
    "    start_t = datetime.now()\n",
    "    x_sample, y_true = Variable(x_sample).cuda(), Variable(y_true).cuda()\n",
    "    \n",
    "    y_pred = model(x_sample)\n",
    "    _, y_pred = torch.max(y_pred.data, 1)\n",
    "    n_correct = torch.sum(y_pred == y_true.data)\n",
    "    n_total_correct += n_correct\n",
    "    \n",
    "    # Time\n",
    "    duration = (datetime.now() - start_t).total_seconds()\n",
    "    total_duration += duration\n",
    "\n",
    "print(f'[{total_duration/N:<6.2}] accuracy:', n_total_correct/N)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
